### BLEU (Bilingual Evaluation Understudy)
**BLEU** is a precision-based metric used for evaluating the quality of text generated by machine translation systems. It compares n-grams (contiguous sequences of n words) of the generated text with n-grams of a reference text.

#### How BLEU is Calculated:
1. **Precision**: Calculate the precision of n-grams (unigrams, bigrams, trigrams, etc.) from the candidate text that appear in the reference text.
2. **Clipping**: To avoid the candidate text inflating precision by repeating words, a **clipping** method limits the number of times an n-gram is counted based on its maximum frequency in the reference text.
3. **Brevity Penalty (BP)**: If the generated text is shorter than the reference text, a brevity penalty is applied to avoid favoring shorter translations.

#### BLEU Formula:

$
\text{BLEU} = BP \times \exp \left( \sum_{n=1}^{N} w_n \log P_n \right)
$

Where:
- \( BP \) is the brevity penalty.
- \( P_n \) is the precision for n-grams.
- \( w_n \) are weights for n-gram precisions (usually evenly distributed).

#### Example BLEU Calculation:
- **Candidate sentence**: "The cat is on the mat."
- **Reference sentence**: "The cat is sitting on the mat."

**Unigram (1-gram) matches**:  
Candidate: `["The", "cat", "is", "on", "the", "mat"]`  
Reference: `["The", "cat", "is", "sitting", "on", "the", "mat"]`  
Unigram match count = 5 (since "sitting" is missing from the candidate)

**Bigrams (2-grams)**:  
Candidate: `["The cat", "cat is", "is on", "on the", "the mat"]`  
Reference: `["The cat", "cat is", "is sitting", "sitting on", "on the", "the mat"]`  
Bigram match count = 3 ("The cat", "cat is", "on the")

**Brevity Penalty (BP)**:  
The candidate length is 6 words, while the reference length is 7. Since the candidate is shorter, the brevity penalty applies:
$
\text{BP} = \exp\left(1 - \frac{7}{6}\right) = 0.93
$

**Final BLEU score**:  
$
\text{BLEU} = 0.93 \times \exp\left( \frac{1}{2} \log(5/6) + \frac{1}{2} \log(3/5) \right)
$

---

### ROUGE-N (Recall-Oriented Understudy for Gisting Evaluation)
**ROUGE-N** is a recall-based metric, focusing on how much of the reference text's n-grams are captured in the generated text.

#### How ROUGE-N is Calculated:
- It is calculated as the recall of n-grams from the reference text that appear in the candidate text.

$
\text{ROUGE-N} = \frac{\text{Number of overlapping n-grams}}{\text{Total n-grams in reference text}}
$

#### Example ROUGE-1 Calculation:
- **Candidate sentence**: "The cat is on the mat."
- **Reference sentence**: "The cat is sitting on the mat."

**Unigram match**:  
Candidate: `["The", "cat", "is", "on", "the", "mat"]`  
Reference: `["The", "cat", "is", "sitting", "on", "the", "mat"]`

Total unigrams in reference = 7  
Matching unigrams = 5 (missing "sitting")

$
\text{ROUGE-1} = \frac{5}{7} = 0.714
#

For **ROUGE-2** (bigrams):
- Candidate bigrams: `["The cat", "cat is", "is on", "on the", "the mat"]`
- Reference bigrams: `["The cat", "cat is", "is sitting", "sitting on", "on the", "the mat"]`

Total bigrams in reference = 6  
Matching bigrams = 3

$
\text{ROUGE-2} = \frac{3}{6} = 0.5
$

### Summary
- **BLEU** is a precision-based metric focusing on how many n-grams from the candidate are present in the reference, with a brevity penalty.
- **ROUGE-N** is a recall-based metric focusing on how much of the reference n-grams are present in the candidate text.
